{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sqlite3\n",
    "import glob\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from helpers_funcs.sql_funcs import fetch_mquote\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Grabbing data from the SQLite Database. \n",
    "datas_conn = sqlite3.connect('stocks_data.db')\n",
    "datas_c = datas_conn.cursor()\n",
    "datas_c.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "current_list = (datas_c.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dates_df_fromdf(symbol_df):\n",
    "    #Generates a Series of the dates and timestamps which correspond to\n",
    "    # Midquote values.\n",
    "    dates= pd.Series(symbol_df['DATE'])\n",
    "    dates = pd.to_timedelta(dates, unit='D') + pd.Timestamp('1960-1-1')\n",
    "    for x in range(len(dates)):\n",
    "        dates[x]=pd.Timestamp(str(dates[x])[:11]+(symbol_df['itime'][x]))\n",
    "        \n",
    "    return dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_KME = pd.read_sql_query('select * from \"{}\"'.format('KME'), datas_conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_KME.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_training_set(stock_data, look_back=5):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(stock_data)-look_back-1):\n",
    "        temp_val = stock_data[i:(i+look_back)]\n",
    "        dataX.append(temp_val)\n",
    "        dataY.append(stock_data[i + look_back])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graphing_wrapper(len_test,len_train,train,test,data_df,title_test,title_train):\n",
    "    fig_MLP, ax_MLP = plt.subplots(figsize=(20,10))\n",
    "    ax_MLP.plot(data_df['mquote'].values[0:(train_size)],color='#225ea8',label='KME Midquote Values')\n",
    "    ax_MLP.plot(train,alpha=0.6,color='#a1dab4',label='Train Prediction')\n",
    "    ax_MLP.set_title(title_test,size=20)\n",
    "    ax_MLP.set_xlabel('Dates')\n",
    "    ax_MLP.set_ylabel('MidQuote')\n",
    "    ax_MLP.legend(prop={'size':18})\n",
    "\n",
    "    ax_MLP.set_xticklabels(['Sep 2000','Nov 2000','Jan 2001','Mar 2001','May 2001','Jul 2001','Sep 2001','Nov 2001'])\n",
    "    fig_MLP2, ax_MLP2 = plt.subplots(figsize=(20,10))\n",
    "    ax_MLP2.plot(data_df['mquote'].values[-(test_size):],color='#225ea8',label='KME Midquote Values')\n",
    "    ax_MLP2.plot(test,color='#41b6c4',alpha=0.6,label='Test Prediction')\n",
    "    ax_MLP2.set_xticklabels(['Dec 2001','Jan 2002','Feb 2002','Mar 2002','Apr 2002','May 2002','Jun 2002','Jul 2002'])\n",
    "    ax_MLP2.set_title(title_train,size=20)\n",
    "    ax_MLP2.set_xlabel('Dates')\n",
    "    ax_MLP2.set_ylabel('MidQuote')\n",
    "    ax_MLP2.legend(prop={'size':18})\n",
    "    \n",
    "    xrand = np.linspace(0,10000,len(data_df['mquote'].values))\n",
    "\n",
    "    fig_MLP3, ax_MLP3 = plt.subplots(figsize=(20,10))\n",
    "    ax_MLP3.plot(xrand,data_df['mquote'].values,color='#225ea8',label='KME Midquote Values')\n",
    "    ax_MLP3.plot(xrand[0:train_size],np.concatenate((train,test))[0:(train_size)],alpha=0.6,color='#a1dab4',label='Train Prediction')\n",
    "    #ax_MLP3.plot(data_df['mquote'].values[-(test_size):],color='#225ea8',label='KME Midquote Values')\n",
    "    ax_MLP3.plot(xrand[-test_size:],np.concatenate((train,test))[-(test_size):],color='#41b6c4',alpha=0.6,label='Test Prediction')\n",
    "    ax_MLP3.set_title('KME Training and Test Prediction from Multilayer Perceptron',size=20)\n",
    "    ax_MLP3.set_xlabel('Dates')\n",
    "    ax_MLP3.set_ylabel('MidQuote')\n",
    "    ax_MLP3.legend(prop={'size':18})\n",
    "    ax_MLP3.set_xticklabels(['Sep 2000','Dec 2000','Mar 2001','Jun 2001','Sep 2001','Dec 2001','Mar 2002','Jun 2002'])\n",
    "\n",
    "    \n",
    "    fig_MLP4, ax_MLP4 = plt.subplots(figsize=(20,10))\n",
    "    ax_MLP4.plot(np.diff(data_df['mquote'].values[0:train_size],n=50),color='#225ea8',label='KME Midquote Values')\n",
    "    ax_MLP4.plot(np.diff(train,n=50),color='#a1dab4',label='Train Prediction')\n",
    "    ax_MLP4.set_xticklabels(['Sep 2000','Nov 2000','Jan 2001','Mar 2001','May 2001','Jul 2001','Sep 2001','Nov 2001'])\n",
    "    ax_MLP4.set_title('Percentage Change in Train Prediction ')\n",
    "    ax_MLP4.set_xlabel('Dates')\n",
    "    ax_MLP4.legend(prop={'size':18})\n",
    "    train_mse = mean_squared_error(data_df['mquote'].values[0:len(train)],train)\n",
    "    print(\"Mean Squared Error of Training = \", train_mse)\n",
    "    \n",
    "    \n",
    "    fig_MLP5, ax_MLP5 = plt.subplots(figsize=(20,10))\n",
    "    ax_MLP5.plot(np.diff(data_df['mquote'].values[-len(test_MLP_predict):],n=50),color='#225ea8',label='KME Midquote Values')\n",
    "    ax_MLP5.plot(np.diff(test,n=50),color='#41b6c4',label='Test Prediction')\n",
    "    ax_MLP5.set_xticklabels(['Dec 2001','Jan 2002','Feb 2002','Mar 2002','Apr 2002','May 2002','Jun 2002','Jul 2002'])\n",
    "    ax_MLP5.set_title('Percentage Change in Test Prediction ')\n",
    "    ax_MLP5.set_xlabel('Dates')\n",
    "    ax_MLP5.legend(prop={'size':18})\n",
    "    test_mse = mean_squared_error(data_df['mquote'].values[-len(test):],test)\n",
    "    print(\"Mean Squared Error of Test = \", test_mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_df=df_KME\n",
    "look_back=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "50s - loss: 0.0719\n",
      "Epoch 2/20\n",
      "53s - loss: 0.0240\n",
      "Epoch 3/20\n",
      "52s - loss: 0.0201\n",
      "Epoch 4/20\n",
      "58s - loss: 0.0185\n",
      "Epoch 5/20\n",
      "67s - loss: 0.0159\n",
      "Epoch 6/20\n",
      "68s - loss: 0.0151\n",
      "Epoch 7/20\n",
      "67s - loss: 0.0143\n",
      "Epoch 8/20\n",
      "67s - loss: 0.0133\n",
      "Epoch 9/20\n",
      "68s - loss: 0.0126\n",
      "Epoch 10/20\n",
      "67s - loss: 0.0121\n",
      "Epoch 11/20\n",
      "68s - loss: 0.0119\n",
      "Epoch 12/20\n",
      "67s - loss: 0.0114\n",
      "Epoch 13/20\n",
      "67s - loss: 0.0113\n",
      "Epoch 14/20\n",
      "67s - loss: 0.0110\n",
      "Epoch 15/20\n",
      "67s - loss: 0.0106\n",
      "Epoch 16/20\n",
      "65s - loss: 0.0103\n",
      "Epoch 17/20\n",
      "64s - loss: 0.0100\n",
      "Epoch 18/20\n",
      "67s - loss: 0.0101\n",
      "Epoch 19/20\n",
      "67s - loss: 0.0098\n",
      "Epoch 20/20\n",
      "67s - loss: 0.0097\n",
      "Train Score: 0.0128722593823\n",
      "Test Score: 0.00150314938883\n"
     ]
    }
   ],
   "source": [
    "train_size=math.floor(len(data_df['mquote'].values)*0.67)\n",
    "test_size = len(data_df['mquote'].values)-train_size\n",
    "train_values = data_df['mquote'].values[0:train_size]\n",
    "test_values = data_df['mquote'].values[train_size:train_size+test_size]\n",
    "train_X,train_Y = create_training_set(train_values,look_back=look_back)\n",
    "test_X,test_Y = create_training_set(test_values,look_back=look_back)\n",
    "model_MLP= Sequential()\n",
    "model_MLP.add(Dense(16, input_dim=look_back, activation='relu'))\n",
    "model_MLP.add(Dense(8,activation='relu'))\n",
    "model_MLP.add(Dense(4,activation='relu'))\n",
    "model_MLP.add(Dense(1))\n",
    "model_MLP.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model_MLP.fit(train_X, train_Y, nb_epoch=20, batch_size=5, verbose=2)\n",
    "train_evaluation  = model_MLP.evaluate(train_X,train_Y,batch_size=5,verbose=0)\n",
    "test_evaluation = model_MLP.evaluate(test_X,test_Y,batch_size=5,verbose=0)\n",
    "print(\"Train Score:\",train_evaluation)\n",
    "print(\"Test Score:\", test_evaluation)\n",
    "train_MLP_predict = model_MLP.predict(train_X)\n",
    "test_MLP_predict = model_MLP.predict(test_X)\n",
    "train_MLP_predict=np.reshape(train_MLP_predict,train_MLP_predict.shape[0])\n",
    "test_MLP_predict=np.reshape(test_MLP_predict,test_MLP_predict.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def consolidate_learning_lstm(initial_neurons,hidden_layers,train_percent,test_percent,data_df,look_back,title_train,title_test):\n",
    "    np.random.seed(7)\n",
    "    scalar = MinMaxScaler(feature_range=(0, 1))\n",
    "    normalized_data = data_df['mquote'].values.reshape(len(data_df['mquote'].values),1)\n",
    "    normalized_data = scalar.fit_transform(normalized_data)\n",
    "    train_size=math.floor(len(normalized_data)*train_percent)\n",
    "    test_size = len(normalized_data)-train_size\n",
    "    train_values = normalized_data[0:train_size]\n",
    "    test_values = normalized_data[train_size:train_size+test_size]\n",
    "    train_X,train_Y = create_training_set(train_values,look_back=look_back)\n",
    "    test_X,test_Y = create_training_set(test_values,look_back=look_back)\n",
    "    train_X = np.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))\n",
    "    test_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))\n",
    "    model_LSTM= Sequential()\n",
    "    model_LSTM.add(LSTM(initial_neurons, input_dim=look_back))\n",
    "    for x in range(hidden_layers):\n",
    "        num =1\n",
    "        if(math.floor(hidden_layers/2)==0):\n",
    "            num =1\n",
    "        else:\n",
    "            num = math.floor(hidden_layers/2)\n",
    "            \n",
    "        model_LSTM.add(Dense(initial_neurons/num,activation='relu'))\n",
    "        initial_neurons = initial_neurons/hidden_layers\n",
    "    model_LSTM.add(Dense(1,activation='sigmoid'))\n",
    "    model_LSTM.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model_LSTM.fit(train_X, train_Y, nb_epoch=50, batch_size=5, verbose=2)\n",
    "    train_evaluation  = model_LSTM.evaluate(train_X,train_Y,batch_size=5,verbose=0)\n",
    "    test_evaluation = model_LSTM.evaluate(test_X,test_Y,batch_size=5,verbose=0)\n",
    "    train_LSTM_predict = model_LSTM.predict(train_X)\n",
    "    test_LSTM_predict = model_LSTM.predict(test_X)\n",
    "    train_LSTM_predict = scalar.inverse_transform(train_LSTM_predict)\n",
    "    train_Y = scalar.inverse_transform(train_Y.reshape(-1, 1))\n",
    "    test_LSTM_predict = scalar.inverse_transform(test_LSTM_predict)\n",
    "    test_Y = scalar.inverse_transform(test_Y.reshape(-1, 1))\n",
    "    #train_LSTM_predict=np.reshape(train_LSTM_predict,train_LSTM_predict.shape[0])\n",
    "    #test_LSTM_predict=np.reshape(test_LSTM_predict,test_LSTM_predict.shape[0])\n",
    "    print((train_LSTM_predict.shape))\n",
    "    \n",
    "    train_predict_plot = np.empty_like(normalized_data)\n",
    "    train_predict_plot[:, :] = np.nan\n",
    "    print((train_predict_plot[look_back:len(train_LSTM_predict)+look_back, :].shape))\n",
    "    train_predict_plot[look_back:len(train_LSTM_predict)+look_back, :] = train_LSTM_predict\n",
    "    test_predict_plot = np.empty_like(normalized_data)\n",
    "    test_predict_plot[:, :] = np.nan\n",
    "    test_predict_plot[len(train_LSTM_predict)+(look_back*2)+1:len(normalized_data)-1, :] = test_LSTM_predict\n",
    "    print(\"Train Score:\",train_evaluation)\n",
    "    print(\"Test Score:\", test_evaluation)\n",
    "    #train_LSTM_predict_mse = mean_squared_error(data_df['mquote'].values[0:len(train_LSTM_predict)],train_predict_plot[~np.isnan(train_predict_plot)])\n",
    "    #test_LSTM_predict_mse = mean_squared_error(data_df['mquote'].values[-(test_LSTM_predict):0],test_predict_plot[~np.isnan(tesPredictPlot)])\n",
    "    #print(\"Training Mean Squared Error\",train_LSTM_predict_mse)\n",
    "    #print(\"Testing Mean Squared Error\",test_LSTM_predict_mse)\n",
    "    #plt.plot(test_predict_plot)\n",
    "    #plt.plot(train_predict_plot)\n",
    "    #graphing_wrapper_LSTM(dates_df_fromdf_2(data_df),train_size,test_size,train_predict_plot,test_predict_plot,data_df,title_train,title_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype object was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-eba1b510ef5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconsolidate_learning_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.65\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.35\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_KME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'KME Training Set Prediction from Longest Short Term Memory'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'KME Test Set Prediction from Longest Short Term Memory'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-8f3ffaa53fe2>\u001b[0m in \u001b[0;36mconsolidate_learning_lstm\u001b[0;34m(initial_neurons, hidden_layers, train_percent, test_percent, data_df, look_back, title_train, title_test)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalized_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtest_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalized_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_training_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlook_back\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlook_back\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_training_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlook_back\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlook_back\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtrain_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f566e6694e13>\u001b[0m in \u001b[0;36mcreate_training_set\u001b[0;34m(stock_data, look_back)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdataX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdataY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlook_back\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "consolidate_learning_lstm(16,2,.65,.35,df_KME,1000,'KME Training Set Prediction from Longest Short Term Memory','KME Test Set Prediction from Longest Short Term Memory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
